{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "%matplotlib notebook\n",
    "import load_raw as Raw\n",
    "import seaborn as sns\n",
    "from scipy.special import expit as logit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import expit as sigmoid # is more stable in case of overflows\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, \\\n",
    "recall_score, precision_score, accuracy_score, confusion_matrix\n",
    "import math\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_csv('/data/ortho/AllPatients.csv',  sep= ';')\n",
    "\n",
    "# x is naar rechts\n",
    "# y is omhoog\n",
    "# z is rotatie????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bereken symmetrie\n",
    "df_cleaned['clavicula_x_dif'] = np.absolute(df_cleaned['clavicula_l_x'] - df_cleaned['clavicula_r_x'])\n",
    "df_cleaned['clavicula_y_dif'] = np.absolute(df_cleaned['clavicula_l_y'] - df_cleaned['clavicula_r_y'])\n",
    "df_cleaned['clavicula_z_dif'] = np.absolute(df_cleaned['clavicula_l_z'] - df_cleaned['clavicula_r_z'])\n",
    "\n",
    "df_cleaned['scapula_x_dif'] = np.absolute(df_cleaned['scapula_l_x'] - df_cleaned['scapula_r_x'])\n",
    "df_cleaned['scapula_y_dif'] = np.absolute(df_cleaned['scapula_l_y'] - df_cleaned['scapula_r_y'])\n",
    "df_cleaned['scapula_z_dif'] = np.absolute(df_cleaned['scapula_l_z'] - df_cleaned['scapula_r_z'])\n",
    "\n",
    "df_cleaned['humerus_x_dif'] = np.absolute(df_cleaned['humerus_l_x'] - df_cleaned['humerus_r_x'])\n",
    "df_cleaned['humerus_y_dif'] = np.absolute(df_cleaned['humerus_l_y'] - df_cleaned['humerus_r_y'])\n",
    "df_cleaned['humerus_z_dif'] = np.absolute(df_cleaned['humerus_l_z'] - df_cleaned['humerus_r_z'])\n",
    "\n",
    "# hulp array, met alle parameters die voor de classifier gebruikt worden, je kan hier alles in doen wat je wilt\n",
    "param = [ \\\n",
    "          'humerus_l_x', 'humerus_l_y', 'humerus_l_z', 'humerus_r_x', 'humerus_r_y', 'humerus_r_z', \\\n",
    "          'clavicula_l_x', 'clavicula_l_y', 'clavicula_l_z', 'clavicula_r_x', 'clavicula_r_y', 'clavicula_r_z', \\\n",
    "          'scapula_l_x', 'scapula_l_y', 'scapula_l_z', 'scapula_r_x', 'scapula_r_y', 'scapula_r_z', \\\n",
    "          'clavicula_x_dif','clavicula_y_dif','clavicula_z_dif', \\\n",
    "          'scapula_x_dif','scapula_y_dif','scapula_z_dif', \\\n",
    "          'humerus_x_dif', 'humerus_y_dif', 'humerus_z_dif'\n",
    "         ]\n",
    "\n",
    "df_cleaned['bias'] = 1\n",
    "\n",
    "# split oorsprong kolom in onderdelen\n",
    "x,y = df_cleaned['Oorsprong'].str.split(\".\").str #Oordprong word vertaald naar een string en wordt gesplits op de punt\n",
    "df_cleaned['cat'],df_cleaned['pat'],df_cleaned['meting'],df_cleaned['oef'] = x.str.split(\"_\").str #4 categorieen gemaakt obv file name\n",
    "df_cleaned['cat'] = [ int(x[3:]) for x in df_cleaned['cat']] #voor elk 3+ element in de kolom wordt vertaald naar een int\n",
    "df_cleaned['meting'] = [ int(x[6:]) for x in df_cleaned['meting']] \n",
    "df_cleaned['oef'] = [ int(x[3:]) for x in df_cleaned['oef']] \n",
    "df_cleaned['pat'] = [ int(x[3:]) for x in df_cleaned['pat']] \n",
    "#na deze regels te hebben uitgevoerd zijn er nieuwe categorieen met ints.\n",
    "\n",
    "df_cleaned['pat'] = df_cleaned['cat']*1000+df_cleaned['pat'] #geef elke patient een uniek nummer\n",
    "\n",
    "#maak boolean kolom per categorie\n",
    "df_cleaned['c4'] = ['Cat4' in vincent for vincent in df_cleaned['Oorsprong']]\n",
    "df_cleaned['c3'] = ['Cat3' in vincent for vincent in df_cleaned['Oorsprong']]\n",
    "df_cleaned['c2'] = ['Cat2' in vincent for vincent in df_cleaned['Oorsprong']]\n",
    "df_cleaned['c1'] = ['Cat1' in vincent for vincent in df_cleaned['Oorsprong']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie om plot van 2 van de variabelen uit de tabel df_cleaned te maken\n",
    "def scatter(param1, param2):\n",
    "    cat4 = df_cleaned.where(df_cleaned['c4'])\n",
    "    cat3 = df_cleaned.where(df_cleaned['c3'])\n",
    "    cat2 = df_cleaned.where(df_cleaned['c2'])\n",
    "    cat1 = df_cleaned.where(df_cleaned['c1'])\n",
    "\n",
    "    plt.plot(cat3[param1], cat3[param2], '.', color='red', markersize=2)\n",
    "    plt.plot(cat1[param1], cat1[param2], '.', color='cyan', markersize=2)\n",
    "    plt.plot(cat2[param1], cat2[param2], '.', color='green', markersize=2)\n",
    "    plt.plot(cat4[param1], cat4[param2], '.', color='blue', markersize=2)\n",
    "    plt.title(param1+'/'+param2)\n",
    "    plt.ylabel(param1)\n",
    "    plt.xlabel(param2);\n",
    "\n",
    "def plot_decision_boundary(theta):\n",
    "    ax = plt.gca()\n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 1),\n",
    "                         np.arange(y_min, y_max, 1))\n",
    "\n",
    "    X = np.matrix(np.vstack([np.ones(xx.shape[0] * xx.shape[1]), xx.ravel(), yy.ravel()])).T\n",
    "    boundary = logit(X * theta)\n",
    "    boundary = boundary.reshape(xx.shape)\n",
    "\n",
    "    ax.contour(xx, yy,\n",
    "           boundary,\n",
    "           levels=[0.5])\n",
    "\n",
    "def logit(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def predict(X, theta):\n",
    "    return h(X, theta) >= 0.5\n",
    "\n",
    "def PlotData(Paramater1, Paramater2):\n",
    "    # maak plotje van 2 variabelen\n",
    "    # Scatter all data in different colors\n",
    "    scatter(param[Paramater1-1],param[Paramater2-1])\n",
    "    \n",
    "    #Draw four decision boundaries\n",
    "    for i in classes:\n",
    "        theta =  Data[class_2b_found]['Model'].coef_.flatten()\n",
    "        selectedtheta = np.array([theta[0], theta[Paramater1], theta[Paramater2]])\n",
    "        selectedtheta = np.reshape(selectedtheta,(3,1))\n",
    "        plot_decision_boundary(selectedtheta)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def CalculatePerc(cat, CountF1, CountF2, CountF3, CountF4):\n",
    "    Count = [CountF1, CountF2, CountF3, CountF4]\n",
    "    cat = int(cat)\n",
    "    perc = 100*Count[cat-1]/np.nansum(Count)\n",
    "    if(math.isnan(perc)):\n",
    "        perc = 0\n",
    "    return perc\n",
    "\n",
    "def CategorieStringToNum(index):\n",
    "    string = str(index)\n",
    "    cat = string[0]\n",
    "    return cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn models (x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.23728238\n",
      "Iteration 2, loss = 0.12182436\n",
      "Iteration 3, loss = 0.08001435\n",
      "Iteration 4, loss = 0.05787191\n",
      "Iteration 5, loss = 0.04231146\n",
      "Iteration 6, loss = 0.03256677\n",
      "Iteration 7, loss = 0.02517756\n",
      "Iteration 8, loss = 0.02251306\n",
      "Iteration 9, loss = 0.02022401\n",
      "Iteration 10, loss = 0.01769724\n",
      "Iteration 11, loss = 0.01420624\n",
      "Iteration 12, loss = 0.01074548\n",
      "Iteration 13, loss = 0.01569606\n",
      "Iteration 14, loss = 0.01001124\n",
      "Iteration 15, loss = 0.01104224\n",
      "Iteration 16, loss = 0.01268172\n",
      "Iteration 17, loss = 0.00688000\n",
      "Iteration 18, loss = 0.00869505\n",
      "Iteration 19, loss = 0.00715325\n",
      "Iteration 20, loss = 0.00966980\n",
      "Iteration 21, loss = 0.00770285\n",
      "Iteration 22, loss = 0.00700489\n",
      "Iteration 23, loss = 0.00646064\n",
      "Iteration 24, loss = 0.00790017\n",
      "Iteration 25, loss = 0.00689971\n",
      "Iteration 26, loss = 0.00571669\n",
      "Iteration 27, loss = 0.00340460\n",
      "Iteration 28, loss = 0.00918025\n",
      "Iteration 29, loss = 0.00772865\n",
      "Iteration 30, loss = 0.00310122\n",
      "Iteration 31, loss = 0.00828089\n",
      "Iteration 32, loss = 0.00351441\n",
      "Iteration 33, loss = 0.00661694\n",
      "Iteration 34, loss = 0.00329195\n",
      "Iteration 35, loss = 0.00441348\n",
      "Iteration 36, loss = 0.00942305\n",
      "Iteration 37, loss = 0.00096772\n",
      "Iteration 38, loss = 0.00402187\n",
      "Iteration 39, loss = 0.00665717\n",
      "Iteration 40, loss = 0.00381546\n",
      "Iteration 41, loss = 0.00530353\n",
      "Iteration 42, loss = 0.00474457\n",
      "Iteration 43, loss = 0.00304996\n",
      "Iteration 44, loss = 0.00217767\n",
      "Iteration 45, loss = 0.00647523\n",
      "Iteration 46, loss = 0.00328048\n",
      "Iteration 47, loss = 0.00037555\n",
      "Iteration 48, loss = 0.01008667\n",
      "Iteration 49, loss = 0.00134434\n",
      "Iteration 50, loss = 0.00430372\n",
      "Iteration 51, loss = 0.00520645\n",
      "Iteration 52, loss = 0.00370280\n",
      "Iteration 53, loss = 0.00107574\n",
      "Iteration 54, loss = 0.00722657\n",
      "Iteration 55, loss = 0.00276503\n",
      "Iteration 56, loss = 0.00040684\n",
      "Iteration 57, loss = 0.00942314\n",
      "Iteration 58, loss = 0.00220594\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.54448562\n",
      "Iteration 2, loss = 0.34530199\n",
      "Iteration 3, loss = 0.25288208\n",
      "Iteration 4, loss = 0.19533632\n",
      "Iteration 5, loss = 0.15937212\n",
      "Iteration 6, loss = 0.13582101\n",
      "Iteration 7, loss = 0.11881516\n",
      "Iteration 8, loss = 0.10473473\n",
      "Iteration 9, loss = 0.09455597\n",
      "Iteration 10, loss = 0.08415167\n",
      "Iteration 11, loss = 0.07980468\n",
      "Iteration 12, loss = 0.06996455\n",
      "Iteration 13, loss = 0.06646436\n",
      "Iteration 14, loss = 0.06237315\n",
      "Iteration 15, loss = 0.05799631\n",
      "Iteration 16, loss = 0.05468035\n",
      "Iteration 17, loss = 0.05056085\n",
      "Iteration 18, loss = 0.04799558\n",
      "Iteration 19, loss = 0.04694999\n",
      "Iteration 20, loss = 0.04427044\n",
      "Iteration 21, loss = 0.04344339\n",
      "Iteration 22, loss = 0.04060796\n",
      "Iteration 23, loss = 0.03906828\n",
      "Iteration 24, loss = 0.03655369\n",
      "Iteration 25, loss = 0.03644311\n",
      "Iteration 26, loss = 0.03362374\n",
      "Iteration 27, loss = 0.03491873\n",
      "Iteration 28, loss = 0.03021730\n",
      "Iteration 29, loss = 0.02858062\n",
      "Iteration 30, loss = 0.03012929\n",
      "Iteration 31, loss = 0.02901514\n",
      "Iteration 32, loss = 0.02675099\n",
      "Iteration 33, loss = 0.02785513\n",
      "Iteration 34, loss = 0.02696869\n",
      "Iteration 35, loss = 0.02660905\n",
      "Iteration 36, loss = 0.02707218\n",
      "Iteration 37, loss = 0.02853164\n",
      "Iteration 38, loss = 0.02159047\n",
      "Iteration 39, loss = 0.02344171\n",
      "Iteration 40, loss = 0.02535315\n",
      "Iteration 41, loss = 0.02257109\n",
      "Iteration 42, loss = 0.02443939\n",
      "Iteration 43, loss = 0.02067186\n",
      "Iteration 44, loss = 0.01858049\n",
      "Iteration 45, loss = 0.02045728\n",
      "Iteration 46, loss = 0.02270805\n",
      "Iteration 47, loss = 0.01792678\n",
      "Iteration 48, loss = 0.02102131\n",
      "Iteration 49, loss = 0.01992400\n",
      "Iteration 50, loss = 0.01655439\n",
      "Iteration 51, loss = 0.02114291\n",
      "Iteration 52, loss = 0.02054389\n",
      "Iteration 53, loss = 0.01811959\n",
      "Iteration 54, loss = 0.01928794\n",
      "Iteration 55, loss = 0.01640038\n",
      "Iteration 56, loss = 0.01632642\n",
      "Iteration 57, loss = 0.01952318\n",
      "Iteration 58, loss = 0.01552431\n",
      "Iteration 59, loss = 0.01734057\n",
      "Iteration 60, loss = 0.01579385\n",
      "Iteration 61, loss = 0.01470632\n",
      "Iteration 62, loss = 0.01823790\n",
      "Iteration 63, loss = 0.01492362\n",
      "Iteration 64, loss = 0.01685565\n",
      "Iteration 65, loss = 0.02034653\n",
      "Iteration 66, loss = 0.01362853\n",
      "Iteration 67, loss = 0.01498916\n",
      "Iteration 68, loss = 0.01514388\n",
      "Iteration 69, loss = 0.01473363\n",
      "Iteration 70, loss = 0.01406698\n",
      "Iteration 71, loss = 0.01313653\n",
      "Iteration 72, loss = 0.01751786\n",
      "Iteration 73, loss = 0.01536237\n",
      "Iteration 74, loss = 0.01101840\n",
      "Iteration 75, loss = 0.01659270\n",
      "Iteration 76, loss = 0.01349161\n",
      "Iteration 77, loss = 0.01092136\n",
      "Iteration 78, loss = 0.01437105\n",
      "Iteration 79, loss = 0.01528695\n",
      "Iteration 80, loss = 0.01239153\n",
      "Iteration 81, loss = 0.01364909\n",
      "Iteration 82, loss = 0.01505307\n",
      "Iteration 83, loss = 0.01280104\n",
      "Iteration 84, loss = 0.01089734\n",
      "Iteration 85, loss = 0.01576490\n",
      "Iteration 86, loss = 0.00886984\n",
      "Iteration 87, loss = 0.01688193\n",
      "Iteration 88, loss = 0.01248064\n",
      "Iteration 89, loss = 0.01213373\n",
      "Iteration 90, loss = 0.00785692\n",
      "Iteration 91, loss = 0.01836110\n",
      "Iteration 92, loss = 0.00815427\n",
      "Iteration 93, loss = 0.01479398\n",
      "Iteration 94, loss = 0.01346434\n",
      "Iteration 95, loss = 0.01099171\n",
      "Iteration 96, loss = 0.01355215\n",
      "Iteration 97, loss = 0.01095009\n",
      "Iteration 98, loss = 0.01157512\n",
      "Iteration 99, loss = 0.01111583\n",
      "Iteration 100, loss = 0.01152931\n",
      "Iteration 101, loss = 0.01470971\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.46513270\n",
      "Iteration 2, loss = 0.30292469\n",
      "Iteration 3, loss = 0.22662439\n",
      "Iteration 4, loss = 0.17612508\n",
      "Iteration 5, loss = 0.14455044\n",
      "Iteration 6, loss = 0.11912463\n",
      "Iteration 7, loss = 0.10363545\n",
      "Iteration 8, loss = 0.09095050\n",
      "Iteration 9, loss = 0.08134942\n",
      "Iteration 10, loss = 0.07301792\n",
      "Iteration 11, loss = 0.06816679\n",
      "Iteration 12, loss = 0.06020194\n",
      "Iteration 13, loss = 0.05795091\n",
      "Iteration 14, loss = 0.05166550\n",
      "Iteration 15, loss = 0.05131084\n",
      "Iteration 16, loss = 0.04618470\n",
      "Iteration 17, loss = 0.04317917\n",
      "Iteration 18, loss = 0.03984677\n",
      "Iteration 19, loss = 0.04080976\n",
      "Iteration 20, loss = 0.03640440\n",
      "Iteration 21, loss = 0.03615397\n",
      "Iteration 22, loss = 0.03138346\n",
      "Iteration 23, loss = 0.03305494\n",
      "Iteration 24, loss = 0.03178477\n",
      "Iteration 25, loss = 0.02939346\n",
      "Iteration 26, loss = 0.02972168\n",
      "Iteration 27, loss = 0.02774675\n",
      "Iteration 28, loss = 0.02575256\n",
      "Iteration 29, loss = 0.02533065\n",
      "Iteration 30, loss = 0.02260017\n",
      "Iteration 31, loss = 0.02583347\n",
      "Iteration 32, loss = 0.02229738\n",
      "Iteration 33, loss = 0.02353919\n",
      "Iteration 34, loss = 0.02254982\n",
      "Iteration 35, loss = 0.02052825\n",
      "Iteration 36, loss = 0.02030788\n",
      "Iteration 37, loss = 0.02308243\n",
      "Iteration 38, loss = 0.01945836\n",
      "Iteration 39, loss = 0.02091539\n",
      "Iteration 40, loss = 0.01752008\n",
      "Iteration 41, loss = 0.02113205\n",
      "Iteration 42, loss = 0.01646236\n",
      "Iteration 43, loss = 0.01961262\n",
      "Iteration 44, loss = 0.01871978\n",
      "Iteration 45, loss = 0.01521571\n",
      "Iteration 46, loss = 0.01938306\n",
      "Iteration 47, loss = 0.01801471\n",
      "Iteration 48, loss = 0.01726129\n",
      "Iteration 49, loss = 0.01564504\n",
      "Iteration 50, loss = 0.01679652\n",
      "Iteration 51, loss = 0.01525054\n",
      "Iteration 52, loss = 0.01790096\n",
      "Iteration 53, loss = 0.01693592\n",
      "Iteration 54, loss = 0.01407921\n",
      "Iteration 55, loss = 0.01664516\n",
      "Iteration 56, loss = 0.01422798\n",
      "Iteration 57, loss = 0.01404656\n",
      "Iteration 58, loss = 0.01766476\n",
      "Iteration 59, loss = 0.01246114\n",
      "Iteration 60, loss = 0.01787749\n",
      "Iteration 61, loss = 0.01566723\n",
      "Iteration 62, loss = 0.01250340\n",
      "Iteration 63, loss = 0.01530648\n",
      "Iteration 64, loss = 0.01288328\n",
      "Iteration 65, loss = 0.01485216\n",
      "Iteration 66, loss = 0.01392361\n",
      "Iteration 67, loss = 0.01270323\n",
      "Iteration 68, loss = 0.01110444\n",
      "Iteration 69, loss = 0.01548729\n",
      "Iteration 70, loss = 0.01507629\n",
      "Iteration 71, loss = 0.01176547\n",
      "Iteration 72, loss = 0.01380768\n",
      "Iteration 73, loss = 0.01455664\n",
      "Iteration 74, loss = 0.00987336\n",
      "Iteration 75, loss = 0.01093173\n",
      "Iteration 76, loss = 0.01091353\n",
      "Iteration 77, loss = 0.01527978\n",
      "Iteration 78, loss = 0.01380798\n",
      "Iteration 79, loss = 0.01147006\n",
      "Iteration 80, loss = 0.01138434\n",
      "Iteration 81, loss = 0.01118878\n",
      "Iteration 82, loss = 0.01044396\n",
      "Iteration 83, loss = 0.01621415\n",
      "Iteration 84, loss = 0.00954819\n",
      "Iteration 85, loss = 0.01193266\n",
      "Iteration 86, loss = 0.01359654\n",
      "Iteration 87, loss = 0.01089744\n",
      "Iteration 88, loss = 0.01046192\n",
      "Iteration 89, loss = 0.00765675\n",
      "Iteration 90, loss = 0.01631838\n",
      "Iteration 91, loss = 0.00979860\n",
      "Iteration 92, loss = 0.00987811\n",
      "Iteration 93, loss = 0.00815175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 94, loss = 0.01213293\n",
      "Iteration 95, loss = 0.01333766\n",
      "Iteration 96, loss = 0.01148712\n",
      "Iteration 97, loss = 0.00834240\n",
      "Iteration 98, loss = 0.01313864\n",
      "Iteration 99, loss = 0.00964470\n",
      "Iteration 100, loss = 0.01178376\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.08308013\n",
      "Iteration 2, loss = 0.01936690\n",
      "Iteration 3, loss = 0.01065739\n",
      "Iteration 4, loss = 0.00635407\n",
      "Iteration 5, loss = 0.00501570\n",
      "Iteration 6, loss = 0.00374457\n",
      "Iteration 7, loss = 0.00354896\n",
      "Iteration 8, loss = 0.00338544\n",
      "Iteration 9, loss = 0.00262755\n",
      "Iteration 10, loss = 0.00284587\n",
      "Iteration 11, loss = 0.00205317\n",
      "Iteration 12, loss = 0.00245188\n",
      "Iteration 13, loss = 0.00214339\n",
      "Iteration 14, loss = 0.00153068\n",
      "Iteration 15, loss = 0.00044087\n",
      "Iteration 16, loss = 0.00182407\n",
      "Iteration 17, loss = 0.00386028\n",
      "Iteration 18, loss = 0.00043740\n",
      "Iteration 19, loss = 0.00179222\n",
      "Iteration 20, loss = 0.00059806\n",
      "Iteration 21, loss = 0.00148980\n",
      "Iteration 22, loss = 0.00302567\n",
      "Iteration 23, loss = 0.00163119\n",
      "Iteration 24, loss = 0.00029729\n",
      "Iteration 25, loss = 0.00069063\n",
      "Iteration 26, loss = 0.00234470\n",
      "Iteration 27, loss = 0.00085443\n",
      "Iteration 28, loss = 0.00009102\n",
      "Iteration 29, loss = 0.00007457\n",
      "Iteration 30, loss = 0.00006819\n",
      "Iteration 31, loss = 0.00006237\n",
      "Iteration 32, loss = 0.00006010\n",
      "Iteration 33, loss = 0.00296264\n",
      "Iteration 34, loss = 0.00209485\n",
      "Iteration 35, loss = 0.00025351\n",
      "Iteration 36, loss = 0.00020275\n",
      "Iteration 37, loss = 0.00253161\n",
      "Iteration 38, loss = 0.00046095\n",
      "Iteration 39, loss = 0.00011541\n",
      "Iteration 40, loss = 0.00007302\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Doe hier alles wat niet in de loop gedaan hoeft te worden\n",
    "    # Bereid de data opslag voor van de loop\n",
    "classes = ['c1','c2','c3','c4']\n",
    "\n",
    "Data = {}\n",
    "for i in classes:\n",
    "    Data[i] = {'Result': None, \n",
    "               'X_Train': None,\n",
    "               'X_test': None, \n",
    "               'y_train': None, \n",
    "               'y_test': None, \n",
    "               'Model': None}\n",
    "    # Einde data opslag voorbereiden\n",
    "\n",
    "\n",
    "\n",
    "#hulpvariabele geeft aan welke categorie we willen onderscheiden\n",
    "Xcolumns = ['bias']\n",
    "Xcolumns.extend(param)\n",
    "\n",
    "for class_2b_found in classes:\n",
    "    classnum = int(class_2b_found[1])\n",
    "    # <-- Begin model moken -->\n",
    "    X = df_cleaned[Xcolumns]\n",
    "    y = df_cleaned[class_2b_found]\n",
    "\n",
    "    # Split Data into a test and train set\n",
    "    Data[class_2b_found]['X_train'], \\\n",
    "    Data[class_2b_found]['X_test'], \\\n",
    "    Data[class_2b_found]['y_train'], \\\n",
    "    Data[class_2b_found]['y_test'] = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "    \n",
    "    # De keuze voor een classifier is \"LogisticRegression\", vandaar dat lr= LogisticRegression\n",
    "    Data[class_2b_found]['Model'] = MLPClassifier(hidden_layer_sizes=(50,40,50),solver='adam',random_state=21,tol=0.00001,learning_rate='adaptive',verbose=10)\n",
    "\n",
    "    # Fit het model, met de train data (80 % )\n",
    "    Data[class_2b_found]['Model'].fit(\\\n",
    "        Data[class_2b_found]['X_train'], Data[class_2b_found]['y_train'])\n",
    "    # <-- Einde model maken -->\n",
    "    \n",
    "    \n",
    "    # <-- Start percentage check -->\n",
    "    # kolom met voorspelling toevoegen aan data\n",
    "    # predict of de test data goed past bij deze LogisticRegression\n",
    "    y_pred = Data[class_2b_found]['Model'].predict(Data[class_2b_found]['X_test'])\n",
    "    \n",
    "    PercentageDf = pd.DataFrame() # Create new dataframe\n",
    "    PercentageDf['index'] = Data[class_2b_found]['X_test'].index # Get the index of the test data\n",
    "    PercentageDf['predict'] = y_pred # Paste the True/False value in the predict column\n",
    "    PercentageDf = PercentageDf.set_index('index') # zet de colom index als de index voor de join hierna\n",
    "    PercentageDf = PercentageDf.join(df_cleaned[['pat', 'cat']]) # Join de pat en cat colom op de index\n",
    "#     PercentageDf['ShouldBe'] = np.where(PercentageDf['cat'] == classnum, True, False)\n",
    "#     PercentageDf['Correct2'] = np.where(PercentageDf['ShouldBe'] == PercentageDf['predict'], True, False)\n",
    "    PercentageDf = PercentageDf.groupby(['pat','predict']).size().to_frame('count').reset_index() # Count the True and False grouped by patients\n",
    "    \n",
    "    \n",
    "    PercentageDf = PercentageDf.pivot(index='pat', columns='predict', values='count') # Creeer een pivot tabel waar de true and false naar colommen gezet worden ipv twee rijen per patient\n",
    "    PercentageDf.columns = ['countF', 'countT'] # Hernoem de colommen naar CountF en CountT\n",
    "    PercentageDf['percentage'] = np.nan_to_num(100 * PercentageDf['countT'] / (PercentageDf['countT'] + PercentageDf['countF'])) # Bereken het percentage dat goed geraden is\n",
    "#     print(PercentageDf)\n",
    "#     ## maak tabel (df4) met per patient het aantal sampels dat true en false gelabeld wordt.\n",
    "\n",
    "#     # aantal false tellen\n",
    "    \n",
    "#     df2 = df_cleaned.groupby(['pat','predict']).size().to_frame('countF').reset_index()\n",
    "#     df2 = df2.where(df2['predict'] == False).dropna()\n",
    "#     df2 = df2.set_index('pat')\n",
    "\n",
    "#     # aantal true tellen\n",
    "#     df3 = df_cleaned.groupby(['pat','predict']).size().to_frame('countT').reset_index()\n",
    "#     df3 = df3.where(df3['predict'] == True).dropna()\n",
    "#     df3 = df3.set_index('pat')\n",
    "\n",
    "#     # join df2 en df3\n",
    "#     df4 = pd.concat([df2, df3], axis=1)\n",
    "#     df4.drop(['predict','predict'], axis=1, inplace=True)\n",
    "#     df4.fillna(0, inplace=True)\n",
    "\n",
    "#     # bereken percentage\n",
    "#     df4['percentage'] = 100 * df4['countT'] / (df4['countT'] + df4['countF'])\n",
    "#     df4.sort_values(by=['percentage'],ascending=False).head(30)\n",
    "\n",
    "    Data[class_2b_found]['result'] = PercentageDf\n",
    "    # <-- End Percentage check -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlotData(5,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informationi matrix c1\n",
      "              pos    neg\n",
      "0  pred pos  6722      8\n",
      "1  pred neg    10  27549\n",
      "\n",
      "recall:  0.9985145573380867\n",
      "precision:  0.9988112927191679\n",
      "accuracy:  0.9994750503076788\n",
      "\n",
      "\n",
      "\n",
      "Informationi matrix c2\n",
      "               pos    neg\n",
      "0  pred pos  13650    138\n",
      "1  pred neg     59  20442\n",
      "\n",
      "recall:  0.995696257932745\n",
      "precision:  0.9899912967798086\n",
      "accuracy:  0.9942547172562629\n",
      "\n",
      "\n",
      "\n",
      "Informationi matrix c3\n",
      "               pos    neg\n",
      "0  pred pos  11688     98\n",
      "1  pred neg     79  22424\n",
      "\n",
      "recall:  0.993286309169712\n",
      "precision:  0.9916850500593924\n",
      "accuracy:  0.9948379946921754\n",
      "\n",
      "\n",
      "\n",
      "Informationi matrix c4\n",
      "              pos    neg\n",
      "0  pred pos  2080      2\n",
      "1  pred neg     1  32206\n",
      "\n",
      "recall:  0.9995194617972128\n",
      "precision:  0.9990393852065321\n",
      "accuracy:  0.9999125083846131\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cat in classes:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "    y_true = Data[cat]['y_test']\n",
    "    y_pred = Data[cat]['Model'].predict(Data[class_2b_found]['X_test'])\n",
    "\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    # tabel printen\n",
    "\n",
    "    print('Informationi matrix %s' % cat)\n",
    "    tab = [[\"pred pos\", TP, FP], [\"pred neg\", FN, TN]]\n",
    "    print(pd.DataFrame(tab, columns=[\"\", \"pos\", \"neg\"]))\n",
    "    print()\n",
    "    print(\"recall: \", recall_score(y_true, y_pred))\n",
    "    print(\"precision: \", precision_score(y_true, y_pred))\n",
    "    print(\"accuracy: \", accuracy_score(y_true, y_pred))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 6.40077847\n",
      "Iteration 2, loss = 5.05010951\n",
      "Iteration 3, loss = 4.11844776\n",
      "Iteration 4, loss = 3.26602346\n",
      "Iteration 5, loss = 2.45296633\n",
      "Iteration 6, loss = 1.71083383\n",
      "Iteration 7, loss = 1.18318715\n",
      "Iteration 8, loss = 0.91092100\n",
      "Iteration 9, loss = 0.77768013\n",
      "Iteration 10, loss = 0.71049456\n",
      "Iteration 11, loss = 0.68381627\n",
      "Iteration 12, loss = 0.67238459\n",
      "Iteration 13, loss = 0.66640138\n",
      "Iteration 14, loss = 0.66241314\n",
      "Iteration 15, loss = 0.65934353\n",
      "Iteration 16, loss = 0.65686348\n",
      "Iteration 17, loss = 0.65486556\n",
      "Iteration 18, loss = 0.65307726\n",
      "Iteration 19, loss = 0.65143676\n",
      "Iteration 20, loss = 0.64992387\n",
      "Iteration 21, loss = 0.64851394\n",
      "Iteration 22, loss = 0.64718658\n",
      "Iteration 23, loss = 0.64594465\n",
      "Iteration 24, loss = 0.64476350\n",
      "Iteration 25, loss = 0.64365106\n",
      "Iteration 26, loss = 0.64258780\n",
      "Iteration 27, loss = 0.64156033\n",
      "Iteration 28, loss = 0.64056369\n",
      "Iteration 29, loss = 0.63956355\n",
      "Iteration 30, loss = 0.63858744\n",
      "Iteration 31, loss = 0.63763950\n",
      "Iteration 32, loss = 0.63671322\n",
      "Iteration 33, loss = 0.63580940\n",
      "Iteration 34, loss = 0.63492375\n",
      "Iteration 35, loss = 0.63404792\n",
      "Iteration 36, loss = 0.63321680\n",
      "Iteration 37, loss = 0.63241309\n",
      "Iteration 38, loss = 0.63169357\n",
      "Iteration 39, loss = 0.63101336\n",
      "Iteration 40, loss = 0.63035499\n",
      "Iteration 41, loss = 0.62973049\n",
      "Iteration 42, loss = 0.62909100\n",
      "Iteration 43, loss = 0.62846373\n",
      "Iteration 44, loss = 0.62783826\n",
      "Iteration 45, loss = 0.62721142\n",
      "Iteration 46, loss = 0.62658476\n",
      "Iteration 47, loss = 0.62596533\n",
      "Iteration 48, loss = 0.62536305\n",
      "Iteration 49, loss = 0.62476731\n",
      "Iteration 50, loss = 0.62417165\n",
      "Iteration 51, loss = 0.62358539\n",
      "Iteration 52, loss = 0.62299876\n",
      "Iteration 53, loss = 0.62242874\n",
      "Iteration 54, loss = 0.62187185\n",
      "Iteration 55, loss = 0.62132494\n",
      "Iteration 56, loss = 0.62078132\n",
      "Iteration 57, loss = 0.62024158\n",
      "Iteration 58, loss = 0.61970299\n",
      "Iteration 59, loss = 0.61917150\n",
      "Iteration 60, loss = 0.61865072\n",
      "Iteration 61, loss = 0.61813330\n",
      "Iteration 62, loss = 0.61762026\n",
      "Iteration 63, loss = 0.61711171\n",
      "Iteration 64, loss = 0.61660657\n",
      "Iteration 65, loss = 0.61610009\n",
      "Iteration 66, loss = 0.61556947\n",
      "Iteration 67, loss = 0.61502787\n",
      "Iteration 68, loss = 0.61448963\n",
      "Iteration 69, loss = 0.61395154\n",
      "Iteration 70, loss = 0.61340674\n",
      "Iteration 71, loss = 0.61286912\n",
      "Iteration 72, loss = 0.61232996\n",
      "Iteration 73, loss = 0.61178128\n",
      "Iteration 74, loss = 0.61122346\n",
      "Iteration 75, loss = 0.61065832\n",
      "Iteration 76, loss = 0.61009357\n",
      "Iteration 77, loss = 0.60954308\n",
      "Iteration 78, loss = 0.60900689\n",
      "Iteration 79, loss = 0.60846397\n",
      "Iteration 80, loss = 0.60791726\n",
      "Iteration 81, loss = 0.60739551\n",
      "Iteration 82, loss = 0.60687349\n",
      "Iteration 83, loss = 0.60635128\n",
      "Iteration 84, loss = 0.60582820\n",
      "Iteration 85, loss = 0.60529671\n",
      "Iteration 86, loss = 0.60475476\n",
      "Iteration 87, loss = 0.60420907\n",
      "Iteration 88, loss = 0.60366034\n",
      "Iteration 89, loss = 0.60310720\n",
      "Iteration 90, loss = 0.60254564\n",
      "Iteration 91, loss = 0.60197918\n",
      "Iteration 92, loss = 0.60139233\n",
      "Iteration 93, loss = 0.60080835\n",
      "Iteration 94, loss = 0.60022626\n",
      "Iteration 95, loss = 0.59963974\n",
      "Iteration 96, loss = 0.59904212\n",
      "Iteration 97, loss = 0.59843247\n",
      "Iteration 98, loss = 0.59781158\n",
      "Iteration 99, loss = 0.59718130\n",
      "Iteration 100, loss = 0.59654397\n",
      "Iteration 101, loss = 0.59588947\n",
      "Iteration 102, loss = 0.59521441\n",
      "Iteration 103, loss = 0.59453080\n",
      "Iteration 104, loss = 0.59384960\n",
      "Iteration 105, loss = 0.59316317\n",
      "Iteration 106, loss = 0.59247974\n",
      "Iteration 107, loss = 0.59178584\n",
      "Iteration 108, loss = 0.59108967\n",
      "Iteration 109, loss = 0.59039508\n",
      "Iteration 110, loss = 0.58970090\n",
      "Iteration 111, loss = 0.58900840\n",
      "Iteration 112, loss = 0.58831320\n",
      "Iteration 113, loss = 0.58761653\n",
      "Iteration 114, loss = 0.58690206\n",
      "Iteration 115, loss = 0.58617265\n",
      "Iteration 116, loss = 0.58544643\n",
      "Iteration 117, loss = 0.58471482\n",
      "Iteration 118, loss = 0.58398385\n",
      "Iteration 119, loss = 0.58324823\n",
      "Iteration 120, loss = 0.58250526\n",
      "Iteration 121, loss = 0.58175856\n",
      "Iteration 122, loss = 0.58100725\n",
      "Iteration 123, loss = 0.58024091\n",
      "Iteration 124, loss = 0.57946635\n",
      "Iteration 125, loss = 0.57869566\n",
      "Iteration 126, loss = 0.57790866\n",
      "Iteration 127, loss = 0.57712062\n",
      "Iteration 128, loss = 0.57632376\n",
      "Iteration 129, loss = 0.57552278\n",
      "Iteration 130, loss = 0.57470669\n",
      "Iteration 131, loss = 0.57388469\n",
      "Iteration 132, loss = 0.57303916\n",
      "Iteration 133, loss = 0.57215691\n",
      "Iteration 134, loss = 0.57126150\n",
      "Iteration 135, loss = 0.57035790\n",
      "Iteration 136, loss = 0.56944430\n",
      "Iteration 137, loss = 0.56854953\n",
      "Iteration 138, loss = 0.56766857\n",
      "Iteration 139, loss = 0.56681908\n",
      "Iteration 140, loss = 0.56596676\n",
      "Iteration 141, loss = 0.56512292\n",
      "Iteration 142, loss = 0.56429820\n",
      "Iteration 143, loss = 0.56347294\n",
      "Iteration 144, loss = 0.56263595\n",
      "Iteration 145, loss = 0.56180278\n",
      "Iteration 146, loss = 0.56097322\n",
      "Iteration 147, loss = 0.56013112\n",
      "Iteration 148, loss = 0.55926554\n",
      "Iteration 149, loss = 0.55839393\n",
      "Iteration 150, loss = 0.55753182\n",
      "Iteration 151, loss = 0.55667652\n",
      "Iteration 152, loss = 0.55581161\n",
      "Iteration 153, loss = 0.55493647\n",
      "Iteration 154, loss = 0.55406236\n",
      "Iteration 155, loss = 0.55318162\n",
      "Iteration 156, loss = 0.55229730\n",
      "Iteration 157, loss = 0.55142119\n",
      "Iteration 158, loss = 0.55053683\n",
      "Iteration 159, loss = 0.54963410\n",
      "Iteration 160, loss = 0.54871741\n",
      "Iteration 161, loss = 0.54779517\n",
      "Iteration 162, loss = 0.54687320\n",
      "Iteration 163, loss = 0.54594714\n",
      "Iteration 164, loss = 0.54502194\n",
      "Iteration 165, loss = 0.54407847\n",
      "Iteration 166, loss = 0.54313980\n",
      "Iteration 167, loss = 0.54219347\n",
      "Iteration 168, loss = 0.54124587\n",
      "Iteration 169, loss = 0.54028201\n",
      "Iteration 170, loss = 0.53928328\n",
      "Iteration 171, loss = 0.53826327\n",
      "Iteration 172, loss = 0.53735010\n",
      "Iteration 173, loss = 0.53642480\n",
      "Iteration 174, loss = 0.53547807\n",
      "Iteration 175, loss = 0.53453954\n",
      "Iteration 176, loss = 0.53360435\n",
      "Iteration 177, loss = 0.53263960\n",
      "Iteration 178, loss = 0.53168220\n",
      "Iteration 179, loss = 0.53070096\n",
      "Iteration 180, loss = 0.52974607\n",
      "Iteration 181, loss = 0.52878180\n",
      "Iteration 182, loss = 0.52781752\n",
      "Iteration 183, loss = 0.52686166\n",
      "Iteration 184, loss = 0.52590256\n",
      "Iteration 185, loss = 0.52494209\n",
      "Iteration 186, loss = 0.52399187\n",
      "Iteration 187, loss = 0.52304762\n",
      "Iteration 188, loss = 0.52212491\n",
      "Iteration 189, loss = 0.52119943\n",
      "Iteration 190, loss = 0.52028119\n",
      "Iteration 191, loss = 0.51937451\n",
      "Iteration 192, loss = 0.51849408\n",
      "Iteration 193, loss = 0.51761017\n",
      "Iteration 194, loss = 0.51673303\n",
      "Iteration 195, loss = 0.51584951\n",
      "Iteration 196, loss = 0.51496415\n",
      "Iteration 197, loss = 0.51407766\n",
      "Iteration 198, loss = 0.51318001\n",
      "Iteration 199, loss = 0.51230252\n",
      "Iteration 200, loss = 0.51141557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/jupyterhub/anaconda/lib/python3.6/site-packages/pandas/core/reshape/merge.py:544: UserWarning: merging between different levels can give an unintended result (2 levels on the left, 1 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('countF', 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('countF', 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fa5148a07050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'percentage'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCalculatePerc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myValueColumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'countF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'countF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'countF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'countF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'percentage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/jupyterhub/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('countF', 1)"
     ]
    }
   ],
   "source": [
    "#collumn namen renamen\n",
    "for num, i in enumerate(classes):\n",
    "    Data[i]['result'].rename(columns={ Data[i]['result'].columns[2]: \"percentagecat%s\" % str(num+1) },inplace=True)\n",
    "\n",
    "\n",
    "#alle resultaten in 1 df joinen\n",
    "resultdf5 = pd.concat([Data['c1']['result'], \\\n",
    "                       Data['c2']['result'], \\\n",
    "                       Data['c3']['result'], \\\n",
    "                       Data['c4']['result']], axis=1, join='inner')\n",
    "\n",
    "# Voorbereiden datafarme, creeer het antwoord (groundtruth) en een bias\n",
    "resultdf5.reset_index(inplace=True)\n",
    "resultdf5['groundtruth'] = resultdf5['pat'].apply(CategorieStringToNum)\n",
    "resultdf5['bias'] = 1\n",
    "resultdf5.set_index('pat', drop=True, inplace=True)\n",
    "\n",
    "param = ['bias','percentagecat1', 'percentagecat2', 'percentagecat3', 'percentagecat4']\n",
    "yValueColumn = 'groundtruth'    \n",
    "X_param = np.matrix(resultdf5[param])\n",
    "y = resultdf5['groundtruth']\n",
    "\n",
    "# Use SKLearn.linear_model\n",
    "model_param = MLPClassifier(hidden_layer_sizes=(50,40,50),solver='adam',random_state=21,tol=0.00001,learning_rate='adaptive',verbose=10)\n",
    "model_param.fit(X_param, y)\n",
    "y_pred = model_param.predict(X_param)\n",
    "\n",
    "# kolom met voorspelling toevoegen aan data)\n",
    "resultdf5['predict'] = y_pred\n",
    "try:\n",
    "    resultdf5['predict_round'] = [int(round(i)) if i <4 and i > 1 else 4 if i > 4 else 1 for i in y_pred]\n",
    "except:\n",
    "    resultdf5['predict_round'] = list(resultdf5['predict'])\n",
    "resultdf5 = resultdf5.drop(['countF', 'countT', ], axis=1)\n",
    "resultdf5.reset_index(inplace=True)\n",
    "\n",
    "## maak tabel (df4) met per patient het aantal sampels dat true en false gelabeld wordt.\n",
    "\n",
    "# aantal false tellen\n",
    "df2 = resultdf5.groupby(['pat','predict_round']).size().to_frame('countF').reset_index()\n",
    "df2.set_index(['pat', 'predict_round'], inplace=True)\n",
    "df2 = df2.unstack(-1)\n",
    "df2 = df2.join(resultdf5[['pat',yValueColumn]].drop_duplicates(subset=['pat', yValueColumn]).set_index('pat'))\n",
    "\n",
    "\n",
    "df2['percentage'] = np.vectorize(CalculatePerc)(df2[yValueColumn], df2['countF', 1],df2['countF', 2],df2['countF', 3],df2['countF', 4])\n",
    "df2.sort_values(by=['percentage'],ascending=False)\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultdf5['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accurracy: \" + str(round((df2.percentage.sum()/100)/df2.percentage.count()*100,2)) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultdf5 = pd.concat([Data['c1']['result'], \\\n",
    "                       Data['c2']['result'], \\\n",
    "                       Data['c3']['result'], \\\n",
    "                       Data['c4']['result']], axis=1, join='inner')\n",
    "\n",
    "# Voorbereiden datafarme, creeer het antwoord (groundtruth) en een bias\n",
    "resultdf5.reset_index(inplace=True)\n",
    "resultdf5['groundtruth'] = resultdf5['pat'].apply(CategorieStringToNum)\n",
    "resultdf5['bias'] = 1\n",
    "resultdf5.set_index('pat', drop=True, inplace=True)\n",
    "\n",
    "resultdf5[['percentagecat1', 'percentagecat2', 'percentagecat3', 'percentagecat4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatemp = df_cleaned.loc[df_cleaned[\"pat\"]==1002]\n",
    "\n",
    "\n",
    "for i in classes:\n",
    "    print(i)\n",
    "    ypredict = Data[i]['Model'].predict_proba(datatemp[Xcolumns])\n",
    "    for x in ypredict[0]:\n",
    "        print(format(x, \".20f\"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things for later\n",
    "## Check most importent parameter\n",
    "\n",
    "    ParamCheck = pd.DataFrame()\n",
    "    ParamCheck['Params'] = list(df_cleaned[Xcolumns])\n",
    "    ParamCheck['Values'] = list(lr.coef_)[0]\n",
    "    ParamCheck['Absolute'] = np.absolute(list(ParamCheck['Values']))\n",
    "    ParamCheck = ParamCheck.sort_values(by=['Absolute'],ascending=False)\n",
    "\n",
    "## Check if plot_decision_boundary is correct\n",
    "    plot_decision_boundary werkt nog via de oude manier, niet via sklearn, en gebruikt misschien niet dezelfdemanier als het model?\n",
    "    Voor nu is dat misschien nog oke, maar als we geen lineare regressie meer gaan doen niet meer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
